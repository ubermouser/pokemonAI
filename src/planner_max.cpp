//#define PKAI_IMPORT
#include "../inc/planner_max.h"

#include <iostream>
#include <iomanip>
#include <sstream>
#include <string>
#include <boost/foreach.hpp>

#include "../inc/evaluator.h"
#include "../inc/pkCU.h"
#include "../inc/fp_compare.h"

#include "../inc/environment_possible.h"
#include "../inc/environment_nonvolatile.h"


void PlannerMax::setEngine(const std::shared_ptr<PkCU>& cu) {
  // copy, rather than share
  Planner::setEngine(std::make_shared<PkCU>(*cu));
  cu_->setAllowInvalidMoves();
}


uint32_t PlannerMax::generateSolution(const ConstEnvironmentPossible& origin) {
  size_t nodesEvaluated;
  uint32_t iBestAction = generateSolution(*cu_, *eval_, origin, agentTeam_, &nodesEvaluated, &results_);

  if (verbose >= 4)
  {
    if (iBestAction != UINT32_MAX)
    {
      const PlannerResult& result = results.back();
      std::clog << "----T" << (agentTeam_==TEAM_A?"A":"B") <<
        ": ply=" << std::setw(2) << 1 << 
        " act=" << std::setw(2) << result.bestAgentAction <<
        " oact=" << std::setw(2) << result.bestOtherAction <<
        " lbFit=" << std::setw(9) << result.lbFitness <<
        " nnod=" << std::dec << nodesEvaluated << 
        "\n"; 
    }
    else
    {
      std::clog << "~~~~T" << (agentTeam_==TEAM_A?"A":"B") <<
        ": NO SOLUTIONS FOUND FOR ANY DEPTH!\n";
    }
  }

  // return best action:
  return (uint32_t)iBestAction;
};


uint32_t PlannerMax::generateSolution(
    PkCU& cu, Evaluator& eval,
    const ConstEnvironmentPossible& origin,
    size_t agentTeam,
    size_t* _nodesEvaluated,
    std::vector<PlannerResult>* results) {
  // a count of the number of nodes evaluated:
  size_t nodesEvaluated = 0;

  // determine the best action based upon the evaluator's prediction:
  fpType bestFitness = -std::numeric_limits<double>::infinity();
  size_t iBestAction = SIZE_MAX;

  for (size_t iAction = 0; iAction != AT_ITEM_USE; ++iAction)
  {
    if (!cu.isValidAction(origin, iAction, agentTeam)) { continue; }

    // produce the resulting state of iAction:
    PossibleEnvironments rEnvP = cu.updateState(
        origin,
        agentTeam==TEAM_A?iAction:AT_MOVE_NOTHING,
        agentTeam==TEAM_B?iAction:AT_MOVE_NOTHING);

    fpType lbFitness = 0.0;
    fpType uncertainty = 1.0;
    for (size_t iEnv = 0; iEnv != rEnvP.size(); ++iEnv) {
      auto cEnvP = rEnvP.at(iEnv);
      if (cEnvP.isPruned()) { continue; }

      fpType cProbability = cEnvP.getProbability().to_double();

      // determine fitness of state generated by iAction:
      EvalResult_t evalResult = eval.calculateFitness(cEnvP.getEnv(), agentTeam);
      lbFitness += evalResult.fitness * cProbability;
      uncertainty -= cProbability;
      ++nodesEvaluated;

      // if there's no possibility this action is the best, do not continue evaluating
      if (mostlyLTE(lbFitness + uncertainty, bestFitness)) { break; }
    }
    // is the returned fitness better than the current best fitness:
    assert(mostlyGTE(lbFitness, 0.0) && mostlyLTE(lbFitness, 1.0));
    if (mostlyGT(lbFitness, bestFitness)) 
    {
      assert(mostlyEQ(uncertainty, 0.0));
      bestFitness = lbFitness;
      iBestAction = iAction;
    }
  }
  
  if (_nodesEvaluated != NULL) { *_nodesEvaluated = nodesEvaluated; }
  if (results != NULL) { 
      results->clear(); results->push_back(PlannerResult(1, iBestAction, -1, bestFitness, bestFitness));
  }
  return (uint32_t) iBestAction;
}
