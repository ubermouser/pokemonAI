//#define PKAI_IMPORT
#include "../inc/planner_max.h"

#include <iostream>
#include <iomanip>
#include <sstream>
#include <string>
#include <boost/foreach.hpp>

#include "../inc/evaluator.h"
#include "../inc/pkCU.h"
#include "../inc/fp_compare.h"

#include "../inc/environment_possible.h"
#include "../inc/environment_nonvolatile.h"

planner_max::planner_max(size_t _engineAccuracy)
	: ident("max_planner-NULLEVAL"),
	results(),
	cu(NULL),
	eval(NULL),
	agentTeam(SIZE_MAX),
	engineAccuracy(_engineAccuracy)
{
}

planner_max::planner_max(const evaluator& evalType, size_t _engineAccuracy)
	: ident(),
	results(),
	cu(NULL),
	eval(evalType.clone()),
	agentTeam(SIZE_MAX),
	engineAccuracy(_engineAccuracy)
{
	{
		std::ostringstream name;
		name << "max_planner-" << eval->getName();
		ident = name.str();
	}
};

planner_max::planner_max(const planner_max& other)
	: ident(other.ident),
	results(),
	cu(other.cu!=NULL?new pkCU(*other.cu):NULL),
	eval(other.eval!=NULL?other.eval->clone():NULL),
	agentTeam(other.agentTeam),
	engineAccuracy(other.engineAccuracy)
{
};

planner_max::~planner_max()
{
	if (eval != NULL) { delete eval; eval = NULL; }
	if (cu != NULL) { delete cu; cu = NULL; }
};

void planner_max::setEvaluator(const evaluator& evalType)
{
	if (eval != NULL) { delete eval; }
	eval = evalType.clone();
	if (cu != NULL) { eval->resetEvaluator(cu->getNV()); };

	{
		std::ostringstream name;
		name << "max_planner-" << eval->getName();
		ident = name.str();
	}
};

void planner_max::setEnvironment(pkCU& _cu, size_t _agentTeam)
{
	agentTeam = _agentTeam;
	if (cu == NULL) { cu = new pkCU(_cu); cu->setAccuracy(engineAccuracy); }
	else { cu->setEnvironment(_cu.getNV()); }
	if (eval != NULL) { eval->resetEvaluator(_cu.getNV()); }
}

bool planner_max::isInitialized() const 
{ 
	if (agentTeam >= 2) { return false; }
	if (cu == NULL) { return false; }
	if (eval == NULL) { return false; }
	if (!eval->isInitialized()) { return false; }

	return true; 
}

uint32_t planner_max::generateSolution(const environment_possible& origin)
{
	size_t nodesEvaluated;
	uint32_t iBestAction = generateSolution(*cu, *eval, origin, agentTeam, &nodesEvaluated, &results);

	if (verbose >= 4)
	{
		if (iBestAction != UINT32_MAX)
		{
			const plannerResult& result = results.back();
			std::clog << "----T" << (agentTeam==TEAM_A?"A":"B") <<
				": ply=" << std::setw(2) << 1 << 
				" act=" << std::setw(2) << result.bestAgentAction <<
				" oact=" << std::setw(2) << result.bestOtherAction <<
				" lbFit=" << std::setw(9) << result.lbFitness <<
				" nnod=" << std::dec << nodesEvaluated << 
				"\n"; 
		}
		else
		{
			std::clog << "~~~~T" << (agentTeam==TEAM_A?"A":"B") <<
				": NO SOLUTIONS FOUND FOR ANY DEPTH!\n";
		}
	}

	// return best action:
	return (uint32_t)iBestAction;
};

uint32_t planner_max::generateSolution(pkCU& cu, evaluator& eval, const environment_possible& origin, size_t agentTeam, size_t* _nodesEvaluated, std::vector<plannerResult>* results)
{
	// a count of the number of nodes evaluated:
	size_t nodesEvaluated = 0;

	// generate array of all possible actions:
	std::vector<environment_possible> rEnvP;

	// determine the best action based upon the evaluator's prediction:
	fpType bestFitness = -std::numeric_limits<double>::infinity();
	size_t iBestAction = SIZE_MAX;

	for (size_t iAction = 0; iAction != AT_ITEM_USE; ++iAction)
	{
		if (!cu.isValidAction(origin.getEnv(), iAction, agentTeam)) { continue; }

		// produce the resulting state of iAction:
		rEnvP.clear();
		cu.updateState(origin.getEnv(), rEnvP, agentTeam==TEAM_A?iAction:AT_MOVE_NOTHING, agentTeam==TEAM_B?iAction:AT_MOVE_NOTHING);

		fpType lbFitness = 0.0;
		fpType uncertainty = 1.0;
		BOOST_FOREACH(const environment_possible& cEnvP, rEnvP)
		{
			if (cEnvP.isPruned()) { continue; }

			fpType cProbability = cEnvP.getProbability().to_double();

			// determine fitness of state generated by iAction:
			evalResult_t evalResult = eval.calculateFitness(rEnvP.front().getEnv(), agentTeam);
			lbFitness += evalResult.fitness * cProbability;
			uncertainty -= cProbability;
			++nodesEvaluated;

			// if there's no possibility this action is the best, do not continue evaluating
			if (mostlyLTE(lbFitness + uncertainty, bestFitness)) { break; }
		}
		// is the returned fitness better than the current best fitness:
		assert(mostlyGTE(lbFitness, 0.0) && mostlyLTE(lbFitness, 1.0));
		if (mostlyGT(lbFitness, bestFitness)) 
		{
			assert(mostlyEQ(uncertainty, 0.0));
			bestFitness = lbFitness;
			iBestAction = iAction;
		}
	}
	
	if (_nodesEvaluated != NULL) { *_nodesEvaluated = nodesEvaluated; }
	if (results != NULL) { results->clear(); results->push_back(plannerResult(1, iBestAction, -1, bestFitness, bestFitness)); }
	return (uint32_t) iBestAction;
}
