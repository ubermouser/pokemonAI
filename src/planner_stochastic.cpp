//#define PKAI_IMPORT
#include "../inc/planner_stochastic.h"

#include <iostream>
#include <iomanip>
#include <sstream>
#include <string>
#include <boost/foreach.hpp>

#include <math.h>
#include <boost/math/special_functions/fpclassify.hpp>

#include "../inc/evaluator.h"
#include "../inc/pkCU.h"
#include "../inc/fp_compare.h"
#include "../inc/roulette.h"

#include "../inc/planner_max.h"

#include "../inc/environment_possible.h"
#include "../inc/environment_nonvolatile.h"

planner_stochastic::planner_stochastic(size_t _engineAccuracy, fpType _temperature, fpType _exploration)
  : ident(),
  results(),
  cu(NULL),
  eval(NULL),
  agentTeam(SIZE_MAX),
  engineAccuracy(_engineAccuracy),
  temperature(_temperature),
  exploration(_exploration)
{
  {
    std::ostringstream name;
    name << "stochastic_planner("
      << temperature << ","
      << exploration << ")-NULLEVAL";
    ident = name.str();
  }
};

planner_stochastic::planner_stochastic(const evaluator& evalType, size_t _engineAccuracy, fpType _temperature, fpType _exploration)
  : ident(),
  results(),
  cu(NULL),
  eval(evalType.clone()),
  agentTeam(SIZE_MAX),
  engineAccuracy(_engineAccuracy),
  temperature(_temperature),
  exploration(_exploration)
{
  {
    std::ostringstream name;
    name << "stochastic_planner("
      << temperature << ","
      << exploration << ")-"
      << eval->getName();
    ident = name.str();
  }
};

planner_stochastic::planner_stochastic(const planner_stochastic& other)
  : ident(other.ident),
  results(),
  cu(other.cu!=NULL?new pkCU(*other.cu):NULL),
  eval(other.eval!=NULL?other.eval->clone():NULL),
  agentTeam(other.agentTeam),
  engineAccuracy(other.engineAccuracy),
  temperature(other.temperature),
  exploration(other.exploration)
{
};

planner_stochastic::~planner_stochastic()
{
  if (eval != NULL) { delete eval; eval = NULL; }
  if (cu != NULL) { delete cu; cu = NULL; }
};

void planner_stochastic::setEvaluator(const evaluator& evalType)
{
  if (eval != NULL) { delete eval; }
  eval = evalType.clone();
  if (cu != NULL) { eval->resetEvaluator(cu->getNV()); };

  {
    std::ostringstream name;
    name << "stochastic_planner("
      << temperature << ","
      << exploration << ")-"
      << eval->getName();
    ident = name.str();
  }
};

void planner_stochastic::setEnvironment(pkCU& _cu, size_t _agentTeam)
{
  agentTeam = _agentTeam;
  if (cu == NULL) { cu = new pkCU(_cu); cu->setAccuracy(engineAccuracy); }
  else { cu->setEnvironment(_cu.getNV()); }
  if (eval != NULL) { eval->resetEvaluator(_cu.getNV()); }
}

bool planner_stochastic::isInitialized() const 
{
  if (!boost::math::isnormal(exploration) || exploration > 1.0) { return false; }
  if (!boost::math::isnormal(temperature)) { return false; } 
  if (agentTeam >= 2) { return false; }
  if (cu == NULL) { return false; }
  if (eval == NULL) { return false; }
  if (!eval->isInitialized()) { return false; }
  
  return true; 
}

uint32_t planner_stochastic::generateSolution(const environment_possible& origin)
{
  size_t nodesEvaluated;
  size_t iBestAction;
  fpType explorationChance = ((fpType) rand()) / ((fpType) RAND_MAX);

  if (explorationChance < exploration) // perform softmax via a boltzmann distribution:
  {
    // generate array of all possible actions:
    std::vector<environment_possible> rEnvP;
    // and store their full unpruned fitnesses:
    std::vector<fpType> fitnesses(AT_ITEM_USE, 0.0);
    fpType bestFitness = -std::numeric_limits<fpType>::infinity();
    fpType worstFitness = std::numeric_limits<fpType>::infinity();
    nodesEvaluated = 0;
    for (size_t iAction = 0; iAction != AT_ITEM_USE; ++iAction)
    {
      if (!cu->isValidAction(origin.getEnv(), iAction, agentTeam)) { continue; }

      // produce the resulting state of iAction:
      rEnvP.clear();
      cu->updateState(origin.getEnv(), rEnvP, agentTeam==TEAM_A?iAction:AT_MOVE_NOTHING, agentTeam==TEAM_B?iAction:AT_MOVE_NOTHING);

      fpType& fitness = fitnesses[iAction];
      BOOST_FOREACH(const environment_possible& cEnvP, rEnvP)
      {
        if (cEnvP.isPruned()) { continue; }

        // determine fitness of state generated by iAction:
        evalResult_t evalResult = eval->calculateFitness(rEnvP.front().getEnv(), agentTeam);
        fitness += evalResult.fitness * cEnvP.getProbability().to_double();
        ++nodesEvaluated;
      }

      if (fitness > bestFitness) { bestFitness = fitness; }
      if (fitness < worstFitness) { worstFitness = fitness; }
      fitness = exp(fitness / temperature);
    }
    // select based on modified fitness:
    iBestAction = roulette<fpType>::select(fitnesses);
    results.clear();
    results.push_back(plannerResult(1, iBestAction, -1, worstFitness, bestFitness));
  }
  else // perform basic max search:
  {
    iBestAction = planner_max::generateSolution(*cu, *eval, origin, agentTeam, &nodesEvaluated, &results);
  }

  if (verbose >= 4)
  {
    if (iBestAction != SIZE_MAX)
    {
      const plannerResult& result = results.back();
      std::clog << "----T" << (agentTeam==TEAM_A?"A":"B") <<
        ": ply=" << std::setw(2) << 1 << 
        " act=" << std::setw(2) << result.bestAgentAction <<
        " oact=" << std::setw(2) << result.bestOtherAction <<
        " lbFit=" << std::setw(9) << result.lbFitness <<
        " ubFit=" << std::setw(9) << result.ubFitness <<
        " nnod=" << std::dec << nodesEvaluated << 
        ((explorationChance < exploration)?" (EXPLORE)":"") <<
        "\n"; 
    }
    else
    {
      std::clog << "~~~~T" << (agentTeam==TEAM_A?"A":"B") <<
        ": NO SOLUTIONS FOUND FOR ANY DEPTH!\n";
    }
  }

  // return best action:
  return (uint32_t)iBestAction;
};